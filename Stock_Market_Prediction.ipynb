{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock_Market_Prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOFwlfocylEoP3un5oKkKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranshu1601/stock_market_Prediction/blob/main/Stock_Market_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "biNAZsVTgZSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf # for fetching the historical stock market data\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import tensorflow as tf # used for creating deep learning piplines \n",
        "\n"
      ],
      "metadata": {
        "id": "gbQzkW68jyRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = yf.download('GOOGL',start= '2018-01-01', interval = '1d')"
      ],
      "metadata": {
        "id": "6XUGwPQFkbbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "iGJ_5azBlwe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "id": "7qr46f8Smtmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the data points based on indexes just for configuration \n",
        "data.sort_index(inplace=True)"
      ],
      "metadata": {
        "id": "Nqqp5qLXoyVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any duplicate index \n",
        "#this will make sure that their is only one record for every date\n",
        "data = data.loc[~data.index.duplicated(keep='first')]"
      ],
      "metadata": {
        "id": "yWUmr2RPpkKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail(3)"
      ],
      "metadata": {
        "id": "yM6kp93vqCzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "yxbWcBQRqHiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here above we see that there isn't any missing values\n",
        "data.describe()\n",
        "#Get the statistics of the data"
      ],
      "metadata": {
        "id": "IOkcBaUVqMuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "#Check the trend in Closing Values\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=data.index , y = data['Close'], mode = 'lines'))\n",
        "fig.update_layout(height = 500 , width = 900 , \n",
        "                                   xaxis_title = 'Date', yaxis_title = 'Close')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "xbfWyv1_uL8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig.add_trace(go.Scatter(x=data.index , y = data['Volume'], mode = 'lines'))\n",
        "fig.update_layout(height = 500 , width = 900 , \n",
        "                                   xaxis_title = 'Date', yaxis_title = 'Volume')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZnFx10pw1fiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "from tqdm.notebook import tnrange"
      ],
      "metadata": {
        "id": "_7m8IeVy2GiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter only required data\n",
        "data = data[['Close' , 'Volume']]\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "dH_19E5o4UaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm the Testing Set length\n",
        "test_length = data[(data.index >= '2021-09-01')].shape[0]"
      ],
      "metadata": {
        "id": "Xsrb5b4u4jBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateFeatures_and_Targets(data , feature_length):\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for i in tnrange(len(data) - feature_length):\n",
        "     X.append(data.iloc[i:i+ feature_length , : ].values)\n",
        "     Y.append(data['Close'].values[i+feature_length])\n",
        "\n",
        "  X= np.array(X)\n",
        "  Y= np.array(Y)\n",
        "\n",
        "  return X, Y    "
      ],
      "metadata": {
        "id": "Z9qk-aL68Pjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , Y = CreateFeatures_and_Targets(data, 32)"
      ],
      "metadata": {
        "id": "QU8Uyfsc9bic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes \n",
        "X.shape , Y.shape"
      ],
      "metadata": {
        "id": "s2SFh-zz9o6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain , Xtest , Ytrain , Ytest = X[:-test_length] , X[-test_length:] , Y[:-test_length], Y[-test_length:]"
      ],
      "metadata": {
        "id": "JUE6aEic-n6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking training dataset shape\n",
        "Xtrain.shape , Ytrain.shape"
      ],
      "metadata": {
        "id": "YjxWLuqT_CRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check testing dataset shape\n",
        "Xtest.shape , Ytest.shape"
      ],
      "metadata": {
        "id": "fCoARPV7_KtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Scaler to scale Vectors with Multiple Dimensions \n",
        "class MultiDimensionScaler():\n",
        "  def __init__(self):\n",
        "    self.scalers = []\n",
        "  def fit_transform(self,X) :\n",
        "    total_dims = X.shape[2]\n",
        "    for i in range(total_dims):\n",
        "      Scaler = MinMaxScaler()\n",
        "      X[: , :, i] = Scaler.fit_transform(X[: , :, i])\n",
        "      self.scalers.append(Scaler)\n",
        "    return X\n",
        "\n",
        "  def transform(self , X):\n",
        "      for i in range(X.shape[2]):\n",
        "          X[:,:,i] = self.scalers[i].transform(X[:,:,i]) \n",
        "      return X   "
      ],
      "metadata": {
        "id": "3jqV7dBa_Us1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature_Scaler = MultiDimensionScaler()\n",
        "Xtrain = Feature_Scaler.fit_transform(Xtrain)\n",
        "Xtest = Feature_Scaler.transform(Xtest)"
      ],
      "metadata": {
        "id": "E_pmRmacYlwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Target_Scaler = MinMaxScaler()\n",
        "Ytrain = Target_Scaler.fit_transform(Ytrain.reshape(-1,1))\n",
        "Ytest = Target_Scaler.transform(Ytest.reshape(-1,1))"
      ],
      "metadata": {
        "id": "823bYyR1khe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_object(obj , name : str ):\n",
        "    pickle_out = open(f\"{name}.pck\",\"wb\")\n",
        "    pickle.dump(obj, pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "def load_object(name:str):\n",
        "    pickle_in = open(f\"{name}.pck\", \"rb\")\n",
        "    data = pickle.load(pickle_in)\n",
        "    return data "
      ],
      "metadata": {
        "id": "l0g1YUmEeLze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your object for future puroses\n",
        "#save_object(Feature_Scaler , \"Feature_Scaler\")\n",
        "#save_object(Target_Scaler , \"Target_Scaler\")\n"
      ],
      "metadata": {
        "id": "GaZK3ZVMe1oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Buliding "
      ],
      "metadata": {
        "id": "YQdMSn0viBwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint , ReduceLROnPlateau\n",
        "\n",
        "save_best = ModelCheckpoint(\"best_weight.h5\",monitor = 'val_loss' , save_best_only=True , save_weights_only = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.25 , patience=5 ,  min_lr = 0.00001,verbose = 1)\n"
      ],
      "metadata": {
        "id": "2C7nzFaCh-UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Dropout , LSTM , Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(512 , return_sequences = True ,  recurrent_dropout=0.1 , input_shape=(32,2))))\n",
        "model.add(LSTM(256, recurrent_dropout=0.1))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64 , activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32,activation='elu'))\n",
        "model.add(Dense(1,activation='linear')) # Final Layer "
      ],
      "metadata": {
        "id": "afQCGX23izKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = tf.keras.optimizer.Adam(learning_rate=0.002)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.002)\n",
        "model.compile(loss='mse',optimizer=optimizer)"
      ],
      "metadata": {
        "id": "gToXwUw4xlMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(Xtrain, Ytrain,\n",
        "                     epochs = 10,\n",
        "                    batch_size = 1,\n",
        "                    verbose = 1,\n",
        "                    shuffle = False,\n",
        "                    validation_data=(Xtest, Ytest),\n",
        "                    callbacks= [reduce_lr , save_best])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjUD1d2Yybts",
        "outputId": "57012d32-3244-4920-b928-bbfa6a9eb718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "891/891 [==============================] - 1032s 1s/step - loss: 0.0032 - val_loss: 0.0025 - lr: 0.0020\n",
            "Epoch 2/10\n",
            "891/891 [==============================] - 1019s 1s/step - loss: 0.0081 - val_loss: 0.0022 - lr: 0.0020\n",
            "Epoch 3/10\n",
            "891/891 [==============================] - 1021s 1s/step - loss: 0.0054 - val_loss: 0.0031 - lr: 0.0020\n",
            "Epoch 4/10\n",
            "350/891 [==========>...................] - ETA: 10:16 - loss: 0.0063"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the best weights\n",
        "model.load_weights(\"best_weight.h5\")"
      ],
      "metadata": {
        "id": "YBbyE36HzGUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = model.predict(Xtest)"
      ],
      "metadata": {
        "id": "QdbIUpPX0_0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = Target_Scaler.inverse_transform(Predictions)\n",
        "Actual = Target_Scaler.inverse_transform(Ytest)"
      ],
      "metadata": {
        "id": "AthHXmCl1JvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions.shape"
      ],
      "metadata": {
        "id": "lQ4vBV0l1bdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = np.squeeze(Predictions , axis =1 )\n",
        "Actual = np.squeeze(Actual , axis = 1)"
      ],
      "metadata": {
        "id": "vP0flcqz1giw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the prediction vs Actual \n",
        "fig =  go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x = data.index[-test_length:] , y= Actual , mode = 'lines' , name='Actual'))\n",
        "fig.add_trace(go.Scatter(x=data.index[-test_length:] , y =Predictions , mode = 'lines', name = 'Predicted' ))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QJmo9Awh1xtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total_features = np.concatenate((Xtrain , Xtest) , axis=0)"
      ],
      "metadata": {
        "id": "hx7EG2qM4jcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Targets = np.concatenate((Ytrain , Ytest) , axis=0) "
      ],
      "metadata": {
        "id": "W9bGlKOe4r-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = model.predict(Total_features)"
      ],
      "metadata": {
        "id": "xwBiM7_J5eTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = Target_Scaler.inverse_transform(Predictions)\n",
        "Actual = Target_Scaler.inverse_transform(Total_Targets)"
      ],
      "metadata": {
        "id": "p2wwc1B-6lCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Predictions = np.squeeze(Predictions , axis =1)\n",
        "Actual = np.squeeze(Actual , axis = 1)"
      ],
      "metadata": {
        "id": "B4lVKPic6x7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the Trend in Volume Trade\n",
        "fig =  go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x = data.index, y= Actual , mode = 'lines' , name='Actual'))\n",
        "fig.add_trace(go.Scatter(x=data.index , y =Predictions , mode = 'lines', name = 'Predicted' ))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GU6PPOYe7Cyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get('https://www.alphavantage.co/query?function=RSI&symbol=GOOGL&interval=daily&time_period=5&series_type=close&apikey=43T9T17VCV2ME4SM')\n",
        "response = response.json()"
      ],
      "metadata": {
        "id": "6bwVpE697oBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.keys()"
      ],
      "metadata": {
        "id": "7JmXG6XU_gIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi_data  = pd.DataFrame.from_dict(response['Technical Analysis: RSI'], orient='index')\n"
      ],
      "metadata": {
        "id": "RBy9tIGY_mg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi_data.head()"
      ],
      "metadata": {
        "id": "s6LPk0zD_1a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi_data = rsi_data[rsi_data.index >= '2018-01-01']"
      ],
      "metadata": {
        "id": "aeAGvaQY_9Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi_data['RSI']=rsi_data['RSI'].astype(np.float64)"
      ],
      "metadata": {
        "id": "XJMzYE0MAGlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rsi_data.head()"
      ],
      "metadata": {
        "id": "wd13QQm1AP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.merge(rsi_data,left_index= True , right_index=True ,  how='inner')"
      ],
      "metadata": {
        "id": "IByrCj67AUkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "rHLJ_pOEAkKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retraining MOdel"
      ],
      "metadata": {
        "id": "O5FO6oVxA3hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm the Testing Set length\n",
        "test_length = data[(data.index >= '2021-09-01')].shape[0]"
      ],
      "metadata": {
        "id": "pP5O67K_A8Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CreateFeatures_and_Targets(data , feature_length):\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for i in tnrange(len(data) - feature_length):\n",
        "     X.append(data.iloc[i:i+ feature_length , : ].values)\n",
        "     Y.append(data['Close'].values[i+feature_length])\n",
        "\n",
        "  X= np.array(X)\n",
        "  Y= np.array(Y)\n",
        "\n",
        "  return X, Y  "
      ],
      "metadata": {
        "id": "2eJyVRnDGdDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , Y = CreateFeatures_and_Targets(data, 32)"
      ],
      "metadata": {
        "id": "JZk2xVEOG5On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes \n",
        "X.shape , Y.shape"
      ],
      "metadata": {
        "id": "lsPMrk-RHW1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain , Xtest , Ytrain , Ytest = X[:-test_length] , X[-test_length:] , Y[:-test_length], Y[-test_length:]"
      ],
      "metadata": {
        "id": "1mfIx07bHlxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking training dataset shape\n",
        "Xtrain.shape , Ytrain.shape"
      ],
      "metadata": {
        "id": "_zmpOZaqJr3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest.shape , Ytest.shape "
      ],
      "metadata": {
        "id": "xINYOFrMJz_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Scaler to scale Vectors with Multiple Dimensions \n",
        "class MultiDimensionScaler():\n",
        "  def __init__(self):\n",
        "    self.scalers = []\n",
        "  def fit_transform(self,X) :\n",
        "    total_dims = X.shape[2]\n",
        "    for i in range(total_dims):\n",
        "      Scaler = MinMaxScaler()\n",
        "      X[: , :, i] = Scaler.fit_transform(X[: , :, i])\n",
        "      self.scalers.append(Scaler)\n",
        "    return X\n",
        "\n",
        "  def transform(self , X):\n",
        "      for i in range(X.shape[2]):\n",
        "          X[:,:,i] = self.scalers[i].transform(X[:,:,i]) \n",
        "      return X   "
      ],
      "metadata": {
        "id": "zhogXBbYJ_Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature_Scaler = MultiDimensionScaler()\n",
        "Xtrain = Feature_Scaler.fit_transform(Xtrain)\n",
        "Xtest = Feature_Scaler.transform(Xtest)"
      ],
      "metadata": {
        "id": "9m_n1a9CKYkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Target_Scaler = MinMaxScaler()\n",
        "Ytrain = Target_Scaler.fit_transform(Ytrain.reshape(-1,1))\n",
        "Ytest = Target_Scaler.transform(Ytest.reshape(-1,1))"
      ],
      "metadata": {
        "id": "DFpQRlAcKYk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_object(obj , name : str ):\n",
        "    pickle_out = open(f\"{name}.pck\",\"wb\")\n",
        "    pickle.dump(obj, pickle_out)\n",
        "    pickle_out.close()\n",
        "\n",
        "def load_object(name:str):\n",
        "    pickle_in = open(f\"{name}.pck\", \"rb\")\n",
        "    data = pickle.load(pickle_in)\n",
        "    return data "
      ],
      "metadata": {
        "id": "t5yMNT4rKYk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your object for future puroses\n",
        "#save_object(Feature_Scaler , \"Feature_Scaler\")\n",
        "#save_object(Target_Scaler , \"Target_Scaler\")\n"
      ],
      "metadata": {
        "id": "0hU4IdhYKYk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint , ReduceLROnPlateau\n",
        "\n",
        "save_best = ModelCheckpoint(\"best_weight.h5\",monitor = 'val_loss' , save_best_only=True , save_weights_only = True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.25 , patience=5 ,  min_lr = 0.00001,verbose = 1)\n"
      ],
      "metadata": {
        "id": "kqLStNcnKYk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Dropout , LSTM , Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Bidirectional(LSTM(512 , return_sequences = True ,  recurrent_dropout=0.1 , input_shape=(32,2))))\n",
        "model.add(LSTM(256, recurrent_dropout=0.1))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64 , activation='elu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32,activation='elu'))\n",
        "model.add(Dense(1,activation='linear')) # Final Layer "
      ],
      "metadata": {
        "id": "uM_hp0YdKYk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = tf.keras.optimizer.Adam(learning_rate=0.002)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.002)\n",
        "model.compile(loss='mse',optimizer=optimizer)"
      ],
      "metadata": {
        "id": "zj2myNeoKYk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(Xtrain, Ytrain,\n",
        "                     epochs = 10,\n",
        "                    batch_size = 1,\n",
        "                    verbose = 1,\n",
        "                    shuffle = False,\n",
        "                    validation_data=(Xtest, Ytest),\n",
        "                    callbacks= [reduce_lr , save_best])"
      ],
      "metadata": {
        "id": "RSDL7ru8KYk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}